---
layout: post
title: "ResNeSt结构分析"
date: 2020-11-17
description: "分析最新ResNeSt网络结构"
tag: 工具
---

1.ResNeSt包含的类
* DropBlock2D 
* GlobalAvgPool2d
* Bottleneck
* RFConv2d
* ResNet

2.ResNeSt与SKNet的区别：
* 在Bottleneck中通过如下的方式构建卷积层;
```python
self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False) # SKNet
group_width = int(planes * (bottleneck_width / 64.)) * cardinality  
self.conv1 = nn.Conv2d(inplanes, group_width, kernel_size=1, bias=False) # ResNeSt
```
* 输出层的定义差异；
```python
out = sum([att*split for (att, split) in zip(atten, splited)])
```

3.类之间的关系
类之间的调用关系如图：
![Figure](/images/posts/markdown/class.png)

4.不同初始参数的网络结构
- resnest50_fast_1s1x64d  
- resnest50_fast_2s1x64d  
- resnest50_fast_4s1x64d
- resnest50_fast_1s2x40d
- resnest50_fast_2s2x40d
- resnest50_fast_4s2x40d
- resnest50_fast_1s4x24d

5.实验准备工作
作者在ResNet网络结构的基础上增加了一些额外的参数，比如redix基数，用于对通道进行缩放操作；groups分组数，用于对通道进行分组卷积；bottleneck_width底部宽度，用于自适应调整输出通道数。分别对三个参数赋予不同的值，构建了不同结构的网络，进行收敛性能/运行时间相关的实验。

6.实验结果及分析
- 训练网络的收敛速度
损失值随训练轮询次数的变化而减小，不同的网络结构收敛速度各异
![Loss](/images/posts/markdown/myplot.png)
- 网络训练的时间性能
不同网络训练轮询次数增加相应的耗时
![Time](/images/posts/markdown/time.png)
- 实验结论
根据实验结果，分析得出以下几点结论：
>(1) ResNeSt在训练网络收敛速度上明显快于MxNet;<br>
>(2) 增加了注意力机制的ResNeSt在训练初期的Loss要明显低于普通版的ResNeSt;<br>
>(3) 保持groups和bottleneck_width不变，增大redix的数值，Loss值下降得越快，但增加到4时这种效应已经饱和；<br>
>(4) 在redix和groups不变的情况下，减小bottleneck_width有利于网络的收敛；<br>
>(5) 在时间消耗上，随着redix数增加（ 其他参数不变），耗时逐渐增加；<br>
>(6) 减小bottleneck_width也可以减少训练耗时；

