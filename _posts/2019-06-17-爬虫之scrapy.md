---
layout: post
title: "爬虫之scrapy"
date: 2019-06-17
description: "scrapy框架介绍"
tag: 网络爬虫
---

### Scrapy爬虫入门

1. 简介

   回答三个问题。scrapy是什么？用来做什么？如何做？首先,scrapy是一个用于爬取网站数据，提取结构性数据而编写的应用框架，它采用了 Twisted异步网络库来处理网络通讯，爬取效率高。在编写爬虫项目前，需要配置相关环境，随后分析网站页面有何规律，比如是采用Get请求还是POST请求，数据是直接通过结构化文件，（如json）发送到前端，还是通过js运行得到的，分析好了这些以后就可以着手开始编写爬虫程序了。

2. 文件结构

   * 整体结构：

     ~~~
     scrapyspider/
         scrapy.cfg          # 项目的配置文件
         scrapyspider/       # 该项目的python模块。之后您将在此加入代码
             __init__.py   
             items.py				# 项目中的item文件
             pipelines.py    # 项目中的pipelines文件
             settings.py     # 项目的设置文件
             spiders/	      # 放置spider代码的目录
                 __init__.py
                 ...
     ~~~

   * 包含组件：
     **引擎(Scrapy)**
       *用来处理整个系统的数据流, 触发事务(框架核心)*
     **调度器(Scheduler)**
       用来接受引擎发过来的请求, 压入队列中, 并在引擎再次请求的时候返回. 可以想像成一个URL（抓取网页的网址或者说是链接）的优先队列, 由它来决定下一个要抓取的网址是什么, 同时去除重复的网址*
     **下载器(Downloader)**
       用于下载网页内容, 并将网页内容返回给蜘蛛(Scrapy下载器是建立在twisted这个高效的异步模型上的)*
     **爬虫(Spiders)**
       爬虫是主要干活的, 用于从特定的网页中提取自己需要的信息, 即所谓的实体(Item)。用户也可以从中提取出链接,让Scrapy继续抓取下一个页面*
     **项目管道(Pipeline)**
       负责处理爬虫从网页中抽取的实体，主要的功能是持久化实体、验证实体的有效性、清除不需要的信息。当页面被爬虫解析后，将被发送到项目管道，并经过几个特定的次序处理数据。*
     **下载器中间件(Downloader Middlewares)**
       位于Scrapy引擎和下载器之间的框架，主要是处理Scrapy引擎与下载器之间的请求及响应。*
     **爬虫中间件(Spider Middlewares)**
       介于Scrapy引擎和爬虫之间的框架，主要工作是处理蜘蛛的响应输入和请求输出。*
     **调度中间件(Scheduler Middewares)**
       介于Scrapy引擎和调度之间的中间件，从Scrapy引擎发送到调度的请求和响应。*

3. 爬虫框架

   * 框架流程图

     ![scrapy](/images/posts/markdown/scrapy.jpg)

   * Scrapy运行流程

     （1）调度器(Scheduler)从 待下载链接 中取出一个链接(URL)

     （2）调度器启动 采集模块Spiders模块

     （3）采集模块把URL传给下载器（Downloader），下载器把资源下载下来

     （4）提取目标数据，抽取出目标对象（Item）,则交给实体管道（item pipeline）进行进一步的处理；比如存入数据库、文本

     （5）若是解析出的是链接（URL）,则把URL插入到待爬取队列当中

4. 爬虫应用

   * 创建项目

     ```Python
     scrapy startproject 项目名称
     ```

   * 启动项目

     ```Python
     scrapy crawl 爬虫名称
     ```

   * 爬虫编写

     4.1. 定义Item

        ```
        import scrapy
        
        class NewsScrapyItem(scrapy.Item):
            # define the fields for your item here like:
            result = scrapy.Field()
        ```

     4.2.  定义管道文件

        ~~~
        import datetime
        import pymysql
        from News_scrapy import settings
        
        class NewsScrapyPipeline(object):
            def __init__(self):
                self.connect = pymysql.connect(
                    host=settings.MYSQL_HOST,
                    db=settings.MYSQL_DBNAME,
                    user=settings.MYSQL_USER,
                    passwd=settings.MYSQL_PASSWORD,
                    use_unicode=True,
                )
                self.cursor = self.connect.cursor()
        
            def open_spider(self, spider):
                pass
        
            def process_item(self, item, spider):
                data_dict = dict(item.get('result'))
                sql = "insert into `表名`" + "(字段1,字段2,...) VALUE (类型1,类型2,...)"
                self.cursor.execute(sql, (...存入数据中的字段...))
                self.connect.commit()
                return item
                
            def close_spider(self, spider):
                self.cursor.close()
                self.connect.close()
        ~~~

     4.3. 配置settings

        ~~~
        BOT_NAME = 'News_scrapy'
        
        SPIDER_MODULES = ['News_scrapy.spiders']
        NEWSPIDER_MODULE = 'News_scrapy.spiders'
        
        MYSQL_HOST = 'localhost'
        MYSQL_DBNAME = 'newsspider'
        MYSQL_USER = 'root'
        MYSQL_PASSWORD = '12345678'
        
        USER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.67 Safari/537.36'
        
        ROBOTSTXT_OBEY = False
        
        ITEM_PIPELINES = {
            'News_scrapy.pipelines.NewsScrapyPipeline': 300,
        }
        ~~~

     4.4. 主爬虫代码

        ```python
        import scrapy
        import json, re
        from ..items import NewsScrapyItem
        
        class NewsSpider(scrapy.Spider):  # 继承 scrapy.Spider 类
            name = 'newsspider'           # 爬虫名称
            def start_requests(self):
                for en_name, para in websites.items():
                    yield scrapy.Request(url=para.get('url'), callback=self.parse, meta={'meta': {'en_name': en_name, 'ch_name': para.get('ch_name'),'types': para.get('types'), 'page_num': para.get('page_num')}})
        
            # 负责解析返回的数据，提取数据(生成item)以及生成需要进一步处理的URL的 Request 对象
            def parse(self, response):
                meta = response.meta.get('meta')
                for ty in meta.get('types'):
                    for num in range(meta.get('page_num')):
                        if meta.get('en_name') == 'qq':
                            page_url = 'https://pacaio.match.qq.com/irs/rcd?cid=146&token=49cbb2154853ef1a74ff4e53723372ce&ext=%s&page=%d' % (ty[0], num)
            		yield scrapy.Request(url=page_url, callback=self.get_news_url_by_page,
                      meta={'meta': {'ch_name': meta.get('ch_name'), 'ty': ty[1]}})
        
        ```



